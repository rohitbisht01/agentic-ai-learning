/* 
  Simple Chatbot Framework with Persistent Memory 

  This is a basic chatbot implementation that supports message exchange between 
  the user and an LLM, with data persistence using MemorySaver and Checkpoint.

  Features:
  - Handles different types of messages:
      - `user` → messages from the user
      - `llm` → responses generated by the model
      - `system` → context or system prompts
      - `tool` → outputs from external tools (if added later)
  - Maintains conversation state as a list of messages.
  - Uses MemorySaver + Checkpoint to persist chat history and restore previous sessions.

  Structure:
  - You can represent messages using either:
      - A custom `Message` interface with `type`, `content`, `timestamp`, and `metadata`
      - Or use the built-in `BaseMessage` type for simplicity.

  Summary:
  This version focuses on a simple, persistent chat loop — a foundation you can 
  later extend with RAG, tool calls, LangSmith tracking, and feedback workflows.
*/

import {
  START,
  END,
  StateGraph,
  Annotation,
  MemorySaver,
} from "@langchain/langgraph";
import { HumanMessage, BaseMessage, AIMessage } from "@langchain/core/messages";
import {
  BedrockRuntimeClient,
  InvokeModelCommand,
} from "@aws-sdk/client-bedrock-runtime";
import dotenv from "dotenv";
import readline from "readline";
dotenv.config();

const ChatBotAnnotation = Annotation.Root({
  messages: Annotation<BaseMessage[]>({
    reducer: (state: BaseMessage[], update: BaseMessage | BaseMessage[]) => {
      return [...state, ...(Array.isArray(update) ? update : [update])];
    },
    default: () => [],
  }),
});

type ChatBotState = typeof ChatBotAnnotation.State;

const REGION = process.env.REGION || "ap-south-1";
const MODEL_ID = process.env.MODEL_ID || "default-model-id";

async function callBedrockLLM(messages: BaseMessage[]): Promise<any> {
  const bedrockClient = new BedrockRuntimeClient({
    region: REGION,
  });

  // Convert all BaseMessages to Bedrock format
  const bedrockMessages = messages.map((msg) => ({
    role: msg._getType() === "human" ? "user" : "assistant",
    content:
      typeof msg.content === "string"
        ? msg.content
        : JSON.stringify(msg.content),
  }));

  console.log("Sending messages to Bedrock:", bedrockMessages);

  const command = new InvokeModelCommand({
    modelId: MODEL_ID,
    accept: "application/json",
    body: JSON.stringify({
      messages: bedrockMessages,
      anthropic_version: "bedrock-2023-05-31",
      temperature: 0.1,
      max_tokens: 4000,
    }),
  });
  const response = await bedrockClient.send(command);
  const output = JSON.parse(new TextDecoder().decode(response.body));

  return output?.content[0]?.text;
}

async function chatNode(state: ChatBotState) {
  const { messages } = state;
  const response = await callBedrockLLM(messages);

  return {
    messages: [new AIMessage(response)],
  };
}

// storing data in memory, not db
const checkPointer = new MemorySaver();

const workflow = new StateGraph(ChatBotAnnotation)
  .addNode("chat_node", chatNode)
  .addEdge(START, "chat_node")
  .addEdge("chat_node", END)
  .compile({ checkpointer: checkPointer });

async function runWorkflow() {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
    terminal: false,
  });

  let threadId = "1";

  console.log("Chatbot started. Type 'quit' or 'exit' to stop.\n");

  const askQuestion = () => {
    rl.question("You: ", async (userInput) => {
      const trimmedInput = userInput.trim();

      if (
        trimmedInput.toLowerCase() === "quit" ||
        trimmedInput.toLowerCase() === "exit"
      ) {
        console.log("Goodbye!");
        rl.close();
        return;
      }

      if (!trimmedInput) {
        askQuestion();
        return;
      }

      const config = { configurable: { thread_id: threadId } };
      const response = await workflow.invoke(
        { messages: [new HumanMessage(trimmedInput)] },
        config
      );

      const lastMessage = response.messages[response.messages.length - 1];
      console.log(`\nAI: ${lastMessage.content}\n`);
      console.log("Final State", response);
      let wholeState = await workflow.getState({
        configurable: { thread_id: "1" },
      });
      console.log("wholeState ", wholeState);

      askQuestion();
    });
  };

  askQuestion();
}

runWorkflow();
